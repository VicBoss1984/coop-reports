<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>University of Guelph Research Assistant Work-Term Report (Summer of 2024)</title>
	<link rel="stylesheet" href="../css/styles.css">
</head>

<body>

	<header>
	<h1>University of Guelph Research Assistant Work-Term Report (Summer of 2024)</h1>
	</header>

	<main>
		<section id="abstract">
			<h2>Abstract/Introduction</h2>

			<p>
				This report will describe my experience working as an undergraduate research assistant (URA) with Doctor Yan in the summer of 2024.
			</p>

			<p>

				I will be highlighting the research project's outline/goals, how it changed throughout the work-term, and draw upon key lessons learned from the work-term.
			</p>

			<p>
				Further, the report will help anyone interested working in the field of computer science research what it is like working as a researcher at the University of Guelph's computer science department.
			</p>

			<p>
				Sections down the line will delve into the learning objectives that I had developed during the work-term, the reflections I made with respect to the learning goals, and details about job description and what I did as a researcher in the School of Computer Science (SoCS) at the University of Guelph.
                	</p>

        	</section>

		<section id="employer-info">
			<h2>Information about the employer</h2>
			<img src="../images/uofg_logo2.jpg" alt="University of Guelph Logo" id="uofg-logo">

			<p>
				The University of Guelph was first established in 1964, but it was not until 1971 that the School of Computer Science (SoCS) (where I worked) was founded. For much of the report, I will be focused on describing SoCS rather than the university as a whole. Nonetheless, the University of Guelph is a research intensive and comprehensive academic institution known for its safe campus, noteworthy presence in various quality science journals, and a relentless focus on student experience.
			</p>

			<p>
				The University of Guelph is, well, in Guelph, Ontario, Canada. The closest and most notable road to the University is Stone Road, which itself is a massive road that leads to a variety of interesting landmarks within Guelph (not including the University, of course).
			</p>

			<p>
				Through various means and methods, the University's, and all of the colleges that it collects under its one roof, ultimate mission is: improve life.

			</p>

			<p>
				How the University improves life comes down to its, as mentioned before, its heavy and quality research output and its emphasis on admitting and educating the best students that Canada and the rest of the world have to offer. Many notable Professors work in the university's various colleges, and many more have an impressive research track record. As for students, look no further than the quality and diverse array of the University's alumni crop to see them working on some of the most exciting and challenging academic and industrial fields in the current world. For instance: Doctor Stefan Kremer is a renowned machine learning expert, and his research into machine learning itself and its practical applications in solving difficult environmental challenges is well documented. In-fact, Doctor Kremer is a University of Guelph alumni, as he did his undergraduate degree in computer science and then went on to obtain his PhD in computer science at the University of Alberta. He now teaches and performs research in the field of machine learning in the School of Computer Science at the University of Guelph.
			</p>

			<p>
				On the academic side, the University earns the right to be labeled as a "comprehensive" university because it houses several academic departments. For instance: the University houses the following great departments: a Psychology department, a Social Sciences department, a Music and Arts department, an Engineering department that houses a wide array of engineering disciplines, a Computer Science department, an Agriculture department, and, finally, a Veterinary School.
			</p>

			<p>
				The School of Computing itself is home to six main branches of computer science research: machine learning, bioinformatics, education, cyber-security, human computer interaction (HCI), and theoretical computer science. My supervisor's, Doctor Yan Yan, primary area of research was bioinformatics, and especially the usage of machine learning to solve complicated and difficult bioinformatics problems. Working in bioinformatics was not novel to me; I worked in an industry position as a first-time co-op student in the summer of 2023 for Norgen Biotek Corporation, but the problems that I solved and the tools that I learned to solve them during my stint as a research assistant to Dr. Yan were novel, interesting, and quite challenging to me. We primarily worked on finding better alternatives and solutions to performing genome-wide association studies (GWAS) using machine learning as the main toolkit to do so (more on this later).
			</p>

			<p>
				While the research intensity of the SoCS department cannot be understated, its teaching prowess is nothing to scoff at. Considering the quantity and quality of the students that learn under the roof and guidance of SoCS, it remains an impressive feat just how effective and efficient the SoCS department is at producing a sublime crop of students that come through its doors. The SoCS department is also home to the excellent SoCS co-operative education department, which handles the co-op needs of computer science students. While the co-op department is, technically, a smaller and a sub-department of the SoCS, its influence is nothing short of incredible. Many students who are registered in the co-op program while doing their undergraduate degree in computing at the University end up being matched with some of the finest and most noteworthy employers in Canada (some even end up working for Facebook, Google, Amazon, Netflix, and Microsoft). I personally know of a decent amount of co-op students in the computer science program who are working for Canada's largest banks, namely: the Royal Bank of Canada (RBC) and the Toronto-Dominion Bank Group (TD Bank).
			</p>

		</section>

		<section id="project-info">
			<h2>Information about the project</h2>
			<img src="../images/project-photo-r3.png" alt="A photograph of different collaborators working on a project together" id="project-photo-r3">

			<p>
				The research project that I worked on with my supervisor, Doctor Yan, was developing a user-friendly, command-line interface (CLI) pipeline for combining traditional GWAS tools (such as GAPIT, TASSEL, and PLINK) with an existing machine learning toolkit specifically designed for GWAS work.

			</p>

			<p>
				The specific machine learning (ML) toolkit that we used and worked on was 'PentaPen'. 'PentaPen' was a machine learning toolkit developed by Nikita Kohli as part of her Master's thesis in data science when she was a student at Thompson Rivers University. The ML toolkit was specifically designed to work with GWAS data, and its development was finished over a few months ago. We inherited this ML toolkit because: according to the Kohli and colleagues (2024) conference paper on the specifics of 'PentaPen', the benefits and precision of using ML tools to perform GWAS work are impossible to deny. Given the importance of the role that GWAS plays in the field of genomic research, especially in the contemporary bioinformatics world with the prevalence of computational methods for finding relevant single-nucleotide polymorphisms (SNPs) in extremely large genomes., finding and developing better ways of performing GWAS is a key and active area of research in bioinformatics.
			</p>

			<p>
				The goal, from the outset, was to combine and simplify the usage of traditional GWAS tools with contemporary and effective ML tools. The means of getting to this goal were defined from the outset when I began working on the project: to use docker, the excellent Golang programming language, and the superb Cobra CLI library to build the best possible CLI interface for this proposed pipeline. The end-goal was to have a working prototype that allows the user, with minimal configuration and installation, to easily run any type of GWAS analysis on their genomic and phenotype input data using the CLI. The results that the user would receive as output from the system would be a collection of identified SNPs that the user would then use as key markers for future genomic research. Some applications of the output that our proposed pipeline would have produced would centre on: biomedical engineering, gene therapy research, and genomic research. Much of the work that our proposed pipeline does is aimed at making the task of finding critical regions of the genome as easy and as feasible as possible.
			</p>

			<p>
				While the theoretical foundation of the work was rock solid, the issues with the existing 'PentaPen' codebase were numerous and very challenging. The issues made it impossible to integrate the 'PentaPen' ML toolkit into a singular, cohesive CLI pipeline, as described earlier. The 'PentaPen' ML toolkit, as a result, required a ton of bug fixes and refactoring to become somewhat stable and usable. Basically, and due to the proliferation of integration and usability issues with the existing 'PentaPen' ML toolkit that we inherited for this project, we spent the vast majority of the project's development testing, finding bugs and defects, and fixing said bugs and defects as they appeared. While the end-result was very promising (and at-least usable): we did manage to get the 'PentaPen' ML toolkit to become much more stable and easier-to-use than when we first inherited and used it, but it still had a small amount of fatal bugs and code defects that made the only remaining two machine learning methods unusable (for context: we got three out of five machine learning models working flawlessly with real-world genomic and phenotype data).
			</p>

			<p>
				At the end of my work-term, myself and Dr. Yan decided to cease work on fixing and testing the 'PentaPen' ML toolkit, and to shift our development resources into researching and integrating existing useful deep-learning methods as a replacement for the 'PentaPen' ML toolkit. The focus of the research project is still the same as before; the only change was we now had to work on finding a suitable deep-learning model for performing GWAS in parallel to the established GWAS tools. Once a deep-learning model is found (which we are very close to doing, as of the time of writing and publishing this report) found, we will integrate it into our existing ecosystem for the Cobra-based CLI pipeline.
			</p>

		</section>

		<section id="goals">
			<h2>Goals and their details</h2>
			<img src="../images/goal-r3.jpg" alt="Stock photo of a nice goal block" id="goal-r3">

			<!-- Learning Goal 1 -->
			<h4>Learning Goal 1</h4>

			<p>
				To develop a robust command-line interface (CLI) program that integrates the 'PentaPen' machine learning toolkit with existing GWAS tools, leveraging Golang and the 'Cobra' library to enhance usability and functionality.
			</p>

			<h4>Action Plan 1</h4>

			<p>
				I will start by thoroughly understanding the requirements of the 'PentaPen' toolkit and the GWAS tools. Then, I will design the architecture of the CLI program, ensuring it is modular and scalable. I will write the core components in Golang using the 'Cobra' library, focusing on creating intuitive command structures and user-friendly error messages.

			</p>

			<h4>Measure of Success 1</h4>

			<p>
				To measure the success of this learning goal, I will conduct unit tests and peer reviews to ensure code quality and functionality. Additionally, I will gather feedback from potential users to refine the interface and address any usability issues.
			</p>

			<h4>Reflection 1</h4>

			<p>
				Due to numerous and significant technical challenges with the existing ‘PentaPen’ codebase, I was unable to focus much of my development time and effort onto the CLI program. Upon testing and debugging the ‘PentaPen’ codebase–for the sole purpose of successfully integrating it fully with the existing Genome-Wide Association Studies (GWAS) tools, it became quite evident that addressing the existing bugs in the ‘PentaPen’ codebase was far more important to the development of the CLI program than building the CLI program itself. Because the CLI program was heavily reliant on the ‘PentaPen’ machine learning (ML) toolkit working flawlessly, its development became pointless if the serious and plentiful technical problems with the ‘PentaPen’ codebase were not addressed fully.
			</p>

			<p>
				Still, I would technically admit that this goal was met–albeit by means of exhaustive testing and debugging on my part with the ‘PentaPen’ codebase, and not by the development of a fully functioning CLI program, as originally intended/stated. Many of the bugfixes that were made necessitated a high amount of ingenuity and problem solving skills to be made, which were demonstrated through the fixing of the many, seemingly impossible to fix, bugs that were discovered during the testing and integration of ‘PentaPen’ into my CLI program.
			</p>

			<p>
				Some of the bugs that were discovered–and subsequently fixed, were hidden in unintentionally well-hidden parts of the ‘PentaPen’ codebase, which is where my problem solving skills came into the fold. Finding the root cause of a bug was not enough; finding a reliable fix that addresses the bug without breaking the remainder of the ‘PentaPen’ ML toolkit’s functionality was very challenging, especially due to the tremendously high coupling present throughout the ‘PentaPen’ codebase and the poor quality of the ‘PentaPen’ codebase altogether. I was still able to find and implement many fixes, but doing so took a substantial amount of development time and resources to make. Nonetheless, I did meet this goal well but through a much different process than what actually happened as described.
			</p>

			<!-- Learning Goal 2 -->
			<h4>Learning Goal 2</h4>

			<p>
				To investigate and implement the best software design patterns for integrating machine learning and GWAS tools into a cohesive pipeline, ensuring seamless data flow and compatibility.
			</p>

			<h4>Action Plan 2</h4>

			<p>
				I will begin by researching existing solutions and best practices for integrating machine learning tools with GWAS tools. This involves reading relevant literature, analyzing current methodologies, and consulting with experts in the field. I will then apply this knowledge to design and implement the integration, ensuring efficient data handling and processing.
			</p>

			<h4>Measure of Success 2</h4>

			<p>
				Success of this learning goal will be measured by the stability and performance of the pipeline, verified through extensive testing and validation with real-world data sets.
			</p>

			<h4>Reflection 2</h4>

			<p>
				Meeting this goal was made possible by the development of my research skills in the field of contemporary machine learning literature. I was able to unearth a ton of different examples of similar, but different machine learning toolkits developed by bioinformatics researchers by going through high-quality bioinformatics journals. Further, I drew upon my experience with developing software at a scalable and industry-standard level when addressing the bugs found in ‘PentaPen’ and by critiquing many of the design decisions in the ‘PentaPen’ codebase to refactor or rewrite it from the ground-up (for the future). The software architecture and code quality critiques were not just isolated to ‘PentaPen’; I also reviewed the codebases of several different ML toolkits for GWAS research and existing non-ML GWAS toolkits for many more concrete examples of where codebases can be significantly improved to create less buggy and more scalable GWAS tools.
			</p>

			<p>
				Nonetheless, the stability and performance of the ‘PentaPen’ codebase was significantly improved by my work on it throughout the work-term. On that note, it would be fair to say that the most challenging part of the proposed pipeline–which has been the ‘PentaPen’ ML toolkit, being made a lot more stable and performant is evidence of myself meeting this goal.
			</p>

			<p>
				For future improvement, I would have liked to make far more rewrites (or refactors) of the existing code in the ‘PentaPen’ rather than focusing solely on the exhaustive testing and debugging of the ‘PentaPen’ codebase. Had I had more time–which would have only been possible if all of the significant bugs with the ‘PentaPen’ ML toolkit were fixed early on in the work-term, which was not technically feasible upon realistic reflection, then I would have focused much of my effort–prior to developing the CLI program, on redesigning and rewriting much of the existing code in the ‘PentaPen’ ML toolkit to be of much higher quality.
			</p>

			<!-- Learning Goal 3 -->
			<h4>Learning Goal 3</h4>

			<p>
				To gain a deep understanding of Golang and R's advanced features and libraries, applying this knowledge to enhance the functionality and performance of the CLI program.
			</p>

			<h4>Action Plan 3</h4>

			<p>
				I will engage in self-directed learning through online tutorials and official and reputable documentation focused on advanced Golang and R features. I will apply this knowledge by incorporating these features into the CLI program, optimizing performance, and adding advanced functionalities.
			</p>

			<h4>Measure of Success 3</h4>

			<p>
				I will measure the success of this learning goal by examining and documenting the program's improved performance metrics, code quality, and the successful implementation of advanced features such as concurrency in Golang and data visualization and manipulation in R.
			</p>

			<h4>Reflection 3</h4>

			<p>
				While the CLI was ultimately delayed in favour of fixing many of the existing issues with the ‘PentaPen’ ML toolkit, I had the honour and privilege of significantly developing my R and Golang programming skills–and my software development skills as a greater whole, through the work that was done in this work-term on this project. Many of the problems that I faced with the ‘PentaPen’ program and the development of the CLI program necessitated a high amount of research and understanding of the R and Golang programming environments and the best software engineering practices/design patterns. I was able to draw upon my experience to become far more comfortable with picking up different and newer programming languages from scratch and making good use of them in a quality fashion. This was a skill that I found to be very enjoyable and relevant to practice and develop throughout the work-term.
			</p>

			<p>
				For instance: I developed an almost complete phenotype file re-formatter in Golang using many of its standard library tools and programming features/rules that–with more time and effort in the future, could be easily extended to be used by other developers in the GWAS area of research. My learning of the basic features of the Golang programming environment was made possible not just by the project mentioned, but also by the self-directed tutorials and smaller-scale educational problems that I was able to complete and solve throughout the work-term, respectively.
			</p>

			<p>
				Another instance of myself meeting this goal: through fixing and exhaustively testing the ‘PentaPen’ technology stack, I was able to develop my knowledge, skills, and aptitude (KSAs) in machine learning development. Much of the R and Python programming environments are very well suited to machine learning research, therefore it was no surprise that I had to leverage much of my existing and developing KSAs with R and Python to better test for and debug issues in the existing ‘PentaPen’ technology stack. Python came in, more precisely, when I was researching different ways other bioinformatics researchers used deep learning in their GWAS work while R was the sole programming tool used in the ‘PentaPen’ codebase.
			</p>

			<!-- Learning Goal 4 -->
			<h4>Learning Goal 4</h4>

			<p>
				To design and develop an innovative solution for reproducibility for the proposed research pipeline so that it is able to be easily and feasibly reproduced and deployed to any UNIX platform–i.e., linux and darwin systems with x86 and ARM architectures.
			</p>

			<h4>Action Plan 4</h4>

			<p>
				To solve this problem, I will investigate what traditional methods for reproducing and deploying software exist in today’s software ecosystem. After finding out what the traditional methods are, I will see if I can use docker to containerize and deploy the entire CLI program/project onto the target UNIX platforms. If docker proves to be a feasible solution, then I would have sidestepped the need for developing a web server, virtual-machine, or other form of software virtualization. By sidestepping the more expensive and less user-friendly solutions mentioned using docker virtualization, I would have developed a creative solution to address the problem of reproducibility and deployment in a user-friendly manner.
			</p>

			<h4>Measure of Success 4</h4>

			<p>
				Success of this learning goal will be measured by the container's ability to run seamlessly on multiple systems without additional configuration, validated through user testing and feedback. Further, if the docker solution makes it easy to use and reproduce the pipeline/project on the user’s personal computer then the goal would also be considered to have been achieved/met.
			</p>

			<h4>Reflection 4</h4>

			<p>
				Since the problems with ‘PentaPen’ halted much of the development of the final CLI program, developing a docker-based solution for containerizing the necessary components in the CLI program into a scalable, reproducible, and easy-to-use format was impossible. As previously mentioned, much of the CLI and docker-based virtualization would be useless if ‘PentaPen’ was not working as it was originally intended.
			</p>

			<p>
				I still met this goal through the creative solutions–and implementations of these solutions, that I was able to come up with when exhaustively testing for bugs in the ‘PentaPen’ ML toolkit and debugging them. However–considering my extensive experience with docker-based software virtualization from my first co-op work-term, putting all of the various dependencies that the CLI program would need to function flawlessly would be quite easy to do, even if it is a creative endeavour. On that same note, a good-quality docker container of the entire CLI program with ‘PentaPen’, the GWAS tools, and all of their dependencies, would have most likely resulted in a much more user-friendly and easier-to-use CLI program.
			</p>

			<p>
				For an instance that demonstrates the ingenuity that I intended to display with the final CLI program: I would have abstracted away the usage of the docker container by making the final CLI program’s frontend function natively on the user’s computing environment–which means that it will not be a part of the docker container while all of the backend technology stack would be fully containerized in the docker container. Doing so, would have necessitated using simple high quality shell scripts strategically within the Golang Cobra-based CLI’s frontend to both supply the user’s choices from the frontend into the docker-containerized backend (where ‘PentaPen’ and the classical GWAS tools reside in a fully integrated fashion) and to execute/run the docker-containerized backend in the user’s native docker runtime environment. Keeping the front-end separate from the containerized backend allows the user to not worry about using docker themselves–just that they must have it installed on their computing environment, and for the CLI’s frontend to perform much better without a layer of software-based virtualization affecting its execution speed.
			</p>

			<!-- Learning Goal 5 -->
			<h4>Learning Goal 5</h4>

			<p>
				To document the CLI program comprehensively, including user guides, installation instructions, and technical documentation, ensuring clear communication to future users and developers.

			</p>

			<h4>Action Plan 5</h4>

			<p>
				I will create detailed documentation for the CLI program, starting with an overview and installation instructions. I will then write user guides with examples and technical documentation covering the program's architecture, codebase, and usage. To ensure clarity, I will seek feedback from peers and potential users, revising the documentation as necessary.
			</p>

			<h4>Measure of Success 5</h4>

			<p>
				Success of this learning goal will be measured by the completeness and clarity of the documentation, verified through peer reviews and user feedback.
			</p>

			<h4>Reflection 5</h4>

			<p>
				I met this goal but through slightly different means–a common theme with the other goals in this document, by the regular written and oral project updates and discussions that I gave and held with my research supervisor, Doctor Yan, throughout the co-op work term. I was able, on many occasions, to effectively convey the true nature of the bugs that I discovered with the existing ‘PentaPen’ codebase via written and oral means, thereby allowing me to communicate technically dense concepts in a much more efficient and accessible format without sacrificing much in the way of technical correctness/precision.
			</p>

			<p>
				Other ways I met this goal were: through the consistent review of the R and Golang programming language standards documents–and any associated manuals for the R and Golang programming languages by the core R and Golang development teams, and the review of the various user manuals of the machine learning libraries/tools used in the ‘PentaPen’ ML toolkit and the classical GWAS tools for testing and debugging purposes in the final CLI program. By reading the various technical documentation available for all of the tools mentioned, I was able to draw upon inspiration from high-quality open-source software documentation when it comes to writing good end-user and developer documentation.
			</p>

			<p>
				Since the CLI was never fully completed, I did not get to use the inspiration I gathered directly on the final CLI’s documentation. Despite that fact, I have demonstrated that learning through the regular project updates–written and oral, that I gave to Doctor Yan throughout the semester. In addition, with Doctor Yan’s help, I developed a poster of the ‘PentaPen’ project and gave a presentation of it in the “Undergraduate Research Summer Showcase” for the College of Engineering and Physical Sciences in the University of Guelph to many non-technical audience members. I succeeded in this practice despite not having a working prototype and final experimental results of the CLI program by taking my existing and thorough knowledge and understanding of the ‘PentaPen’ ML toolkit and sharing it with the audience members during my poster presentations. This practice was quite a valuable learning experience in communicating dense technical topics to non-technical audiences in a real-world setting, which is why it is mentioned here.
			</p>
		</section>

		<section id="project-description-job-posting">
			<img src="../images/dna-image1.jpg" alt="A photo of the DNA double-helix" id="dna-photo1">
			<h2>Project description (exactly as it appeared on the original job posting)</h2>

			<p>
				With the development of next generation sequencing, vast amount of whole-genome data becomes available nowadays. Genome-Wide Association Studies (GWAS) have served as primary methods for the past decade for identifying associations between genetic variants and traits or diseases (also known as phenotype). The most often used genetic variants are Single Nucleotide Polymorphisms (SNPs), which are changes of single DNA base-pairs. Such data has brought complex challenges to current whole-genome studies as the data is sparse and in high dimension.
			</p>

			<p>
				Features selection on the whole-genome SNP data can potentially solve this issue. By selecting a reduced number of SNPs (features) with significantly larger effects compared to other SNPs, researchers can apply existing methods on the most promising SNPs. Penalized regression models including Least Absolute Shrinkage and Selection Operator (LASSO) and its variations are proven to be very well-suited for sparse problems often serving as a good way to select significant features. Currently, we have studied several methods on a model plant, and we plan to expand the study to further validate the findings and improve the methods.
			</p>

		</section>

		<section id="job-description">
			<h2>Job Description</h2>
			<img src="../images/job-descriptions-photo.jpg" alt="A photo of a binder with job description sheets inside it" id="job-description-photo">

			<p>
				Most of the work that I did was in Golang and R. Golang was chosen as the language of choice for the CLI and R was chosen as the language of choice for its ease-of-use with linear regression ML models. I worked with a wide variety of models in R, but I will likely be using Python to develop deep learning models in the future. Nonetheless–and regardless of which languages were used in this project, I designed, built, and fixed software in many different situations/contexts when difficult problems presented themselves.
			</p>

			<ul>
				<li>Collect additional SNP datasets from online database and test the existing developed methods to validate the findings.</li>
				<li>Develop a tool/pipeline to automatically run multiple methods on the same dataset and compare the performance.</li>
				<li>Analyze the tool to improve the performance (e.g. computational speed, capacity of handling big data, etc.).</li>
				<li>Develop detailed documentation and the user manual of the tool and publish the tool to the online repository, e.g. GitHub.</li>
				<li>Explore the literature to find other methods that could be used for feature selection, e.g. deep learning methods, and apply them on the SNP datasets to examine the performance.</li>
			</ul>

			<p>
				The usual areas that are relevant for the main software development tasks were prominent here: designing good software, critiquing bad software and fixing it, testing software for all of its use-cases, debugging and troubleshooting, learning and picking up new programming languages and libraries from scratch, and writing and designing good code. The numerous challenges I came across in this project offered ample opportunity to practice the mentioned core skills, which is where the main net positive that dealing with the challenges lies.
			</p>

			<p>
				My strong academic and prior industry background served me well in this work-term. I was able to draw upon my experiences, knowledge, skills, and aptitude to serve the project's development. I found the process of applying what I have learned in school and industry and reflecting on it immensely satisfying.
			</p>

		</section>

		<section id="conclusions">
			<h2>Conclusions</h2>

			<p>
				To sum everything up, I would say–despite the project having been shifted to use deep learning methods instead of the group-based linear regression methods, the project was challenging and satisfying to work on. My favorite part was developing a phenotype re-formatter in Golang and a genotype file trimmer in Python. Having the ability to design and build my own software from scratch to solve certain problems that have not been solved before was a great feeling to have.
			</p>

			<p>
				Working in the research field of computer science for the first time was a gratifying and eye-opening experience. I was exposed to new ways of developing software, and spent a lot of time finding and reading contemporary literature about the same problems and areas of research that I am working on at the time.
			</p>

			<p>
				My growth during this work-term was quite rapid. I was able to pick up two new programming languages–namely, Golang and R, to help me solve problems in an efficient and effective way. Prior to how the work-term, I can safely say that I knew next-to-nothing about machine learning. Now, I have learned so much–and am aware and excited about the fact I still have so much left to learn about machine learning and bioinformatics. I owe all of this growth to the superb supervision and support that Doctor Yan provided throughout the summer work-term.
			</p>

		</section>

        <section id="acknowledgments">
            <h2>Acknowledgments</h2>

	    <p>
                I would like to acknowledge and thank my supervisor: Dr. Yan Yan in the School of Computer Science at the University of Guelph, for having faith in me and choosing me for the research assistant position. Her support, especially through moments that seemed impossible to overcome, made getting to the finish line of this summer work-term possible and satisfying. Everything I have now–the growth, the knowledge, skills, and aptitude relevant to bioinformatics and machine learning, is possible thanks to her.
            </p>

	    <p>
		I would also like to thank the University of Guelph for providing funding for the project and for allowing me to continue working with Dr. Yan for the fall 2024 semester as a research assistant. Thanks to the university, I am able to attempt–and hopefully finish, the CIS4900 (undergraduate research project I) course under Dr. Yan's supervision.
	    </p>

        </section>

    </main>

    <footer>
        <p>Website made by Ali Riayde Jawad - Computer Science (Co-op) Student at the University of Guelph</p>
    </footer>

</body>

</html>
